@INPROCEEDINGS{Li2310:Efficacy,
AUTHOR="Kunyang Li and Kyle D Domico and Jean-Charles {Noirot Ferrand} and Patrick
McDaniel",
TITLE="The Efficacy of {Transformer-Based} Adversarial Attacks in Security Domains",
BOOKTITLE="MILCOM 2023 - Workshop on Artificial Intelligence for Cyber (MILCOM 2023 -
Workshop on AI for Cyber)",
ADDRESS="Boston, USA",
PAGES=6,
DAYS=30,
MONTH=oct,
YEAR=2023,
ABSTRACT="Today, the security of many domains rely on the use of Machine Learning to
detect threats, identify vulnerabilities, and safeguard systems from
attacks. Recently, transformer architectures have improved the
state-of-the-art performance on a wide range of tasks such as malware
detection and network intrusion detection. But, before abandoning current
approaches to transformers, it is crucial to understand their properties
and implications on cybersecurity applications. In this paper, we evaluate
the robustness of transformers to adversarial samples for system defenders
(i.e., resiliency to adversarial perturbations generated on different types
of architectures) and their adversarial strength for system attackers
(i.e., transferability of adversarial samples generated by transformers to
other target models). To that effect, we first fine-tune a set of
pre-trained transformer, Convolutional Neural Network (CNN), and hybrid (an
ensemble of transformer and CNN) models to solve different downstream
image-based tasks. Then, we use an attack algorithm to craft 19,367
adversarial examples on each model for each task. The transferability of
these adversarial examples is measured by evaluating each set on other
models to determine which models offer more adversarial strength, and
consequently, more robustness against these attacks. We find that the
adversarial examples crafted on transformers offer the highest
transferability rate (i.e., 25.7\% higher than the average) onto other
models. Similarly, adversarial examples crafted on other models have the
lowest rate of transferability (i.e., 56.7\% lower than the average) onto
transformers. Our work emphasizes the importance of studying transformer
architectures for attacking and defending models in security domains, and
suggests using them as the primary architecture in transfer attack
settings."
}